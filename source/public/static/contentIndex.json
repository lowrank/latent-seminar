{"2022-Fall/on-\"sum-of-squares\"":{"title":"on \"sum of squares\"","links":[],"tags":["math-NA","math-RA"],"content":"Motivation\nThoughts\nNotes\n\n\n\nLinks\n\n\n"},"2022-Fall/on-Gaussian-Correlation-Inequality":{"title":"on Gaussian Correlation Inequality","links":[],"tags":["math-PR","math-CA","math-CO"],"content":"\n\n                  \n                  Info\n                  \n                \n\nThe Gaussian Correlation Inequality was proved in 2014 (arXiv: 1408.1028). The interesting story can be found in Quanta Magazine.\n\n\nüè∑Ô∏è Introduction\nThe inequality is to show the Gaussian measure \\mu on centrally symmetric convex sets A and B satisfies\n\\mu(A\\cap B)\\ge \\mu(A)\\mu(B).\nThat is to say, if a dart hits the wall with standard Gaussian distribution, suppose two targets are centrally symmetric convex sets A,  B, then hitting both targets with one dart is easier than hitting A with the first dart and hitting B with the second, vice versa.\nThe proof of inequality is simple and elegant. I think there are a few keys in the proof which are insightful (that is why this note exists üòÅ).  The first observation is the following.\n\n\n                  \n                  Observation \n                  \n                \n\nA centrally symmetric convex closed set can be formed by the intersection of countable symmetric strips. 1\n\n\nThis observation of ‚Äústrips‚Äù is natural, since a convex symmetric body can be approximated by a sequence of convex, symmetric polytopes. Moreover, convex, symmetric polytopes are just slices of the unit cube in a higher dimension satisfying the constraints \\{|\\langle x, v_i \\rangle |\\le 1\\} for i=1,2,\\cdots, k. Therefore, the problem can be reduced to proving\n\n\n                  \n                  Gaussian Correlation Theorem \n                  \n                \n\n\\mathbb{P}\\left(\\bigcap_{i=1}^n A_i\\right) \\ge \\mathbb{P}\\left(\\bigcap_{i=1}^k A_i\\right) \\mathbb{P}\\left(\\bigcap_{i=k+1}^n A_i\\right),\nwhere A_i = \\{|X_i|\\le x_i\\}, i=1,2,\\cdots, n and (X_1, \\cdots, X_n)\\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_n).\n\n\nThe special case k=1 was proved by the following theorem 2.\n\n\n                  \n                  Theorem (Khatri) \n                  \n                \n\nLet \\{X_i\\}_{1\\le i\\le n} be a jointly Gaussian random variables with mean zero. Then\n\\mathbb{P}(\\max_{1\\le i\\le n} |X_i| \\le 1) \\ge \\mathbb{P}(|X_1|\\le 1)\\, \\mathbb{P}(\\max_{2\\le i \\le n} |X_i| \\le 1).\n\n\nüåäMultivariate Gamma-type distribution\nThe set \\{|X_i|\\le 1\\} is better described by chi-squared distribution or Gamma distribution. Surprisingly, the multivariate Gamma distributions on \\mathbb{R}^n have several (non-equivalent) definitionsü§£.\nThe \\Gamma(\\alpha, R) distribution is defined as follows.\n\n\n                  \n                  Definition \n                  \n                \n\nIf the random vector X = (X_1, \\cdots, X_n) satisfies the Laplace transform\n\\mathbb{E}[\\exp\\left(-\\langle s, X\\rangle \\right)] = \\frac{1}{|I + R \\operatorname{diag}(s)|^{\\alpha}},\nthen it obeys the \\Gamma(\\alpha, R) distribution.\n\n\n\n\n                  \n                  Example\n                  \n                \n\nIf \\alpha=\\frac{1}{2} and X\\sim \\chi^2_n or X\\sim \\Gamma(\\alpha=\\frac{n}{2}, \\theta=2), then the covariance matrix R = 2I_n and\n\\mathbb{E}[\\exp(-\\langle s, X\\rangle)] = \\prod_{i=1}^n \\mathbb{E}[\\exp(-s_i X_i)] = \\prod_{i=1}^n \\int_0^{\\infty} \\frac{x^{-1/2} e^{-x/2}}{2^{1/2}\\Gamma(\\frac{1}{2})} e^{-s_i x} dx = \\prod_{i=1}^n \\frac{1}{\\sqrt{2s_i + 1}}.\n\n\nNote, not all values of \\alpha suffice to produce an admissible distribution. Some possible values of \\alpha are given in3.\n\n\n                  \n                  Example\n                  \n                \n\nSuppose X_i\\sim \\mathcal{N}(\\mathbf{0}_p, R), then the Wishart matrix S = \\sum_{i=1}^n X_i X_i^T \\sim \\mathcal{W}_p(n, R). The diagonal part of  S is \\sum_{i=1}^n X_i\\odot X_i by Hadamard product. Then, the Laplace transform (or equivalently moment generating function) is\n\\begin{aligned}\n\\mathcal{L}(\\operatorname{diag}(S)) &amp;= \\int dX_1 \\cdots dX_n \\;p(X_1, X_2, \\cdots, X_n) e^{-s^T (\\sum_{i=1}^n X_i\\odot X_i)} \\\\&amp;= \\left[ \\int dX \\frac{1}{\\sqrt{2\\pi}} |R|^{-1/2} \\exp\\left(-\\frac{1}{2} X^T R^{-1} X\\right) \\exp(-s^T (X\\odot X))\\right]^n \\\\\n&amp;= \\left[\\int dX \\frac{1}{\\sqrt{2\\pi}} |R|^{-1/2} \\exp\\left(-\\frac{1}{2} X^T (R^{-1} + 2\\operatorname{diag}(s)) X\\right) \\right]^n\\\\\n&amp;= |R|^{-n/2} |( R^{-1} + 2\\operatorname{diag}(s) )|^{-n/2}\\\\\n&amp;= |I + 2R \\operatorname{diag}(s)|^{-n/2}.\n\\end{aligned}\nTherefore, all 2\\alpha\\in \\mathbb{N} are admissible values.\n\n\nüåµVariational Technique\n\n\nIn order to distinguish the dependence and independence, it is very common to introduce the correlation matrix for the n dimensional vector X, that is, C(\\tau) = [C_{11}, \\tau C_{12}; \\tau C_{21} ,C_{22}] in the spirit of variational method, then the left-hand and right-hand sides of the desired inequality are referring the case \\tau = 1 and \\tau = 0. It equivalently means the function\n\n\n\\tau \\mapsto \\mu(Z_i(\\tau)\\le s_i, i=1,2\\dots, n)\nis non-decreasing in \\tau, where Z_i = \\frac{X_i^2}{2} and s_i = \\frac{t_i^2}{2}. Let f(z,\\tau) be the joint distribution‚Äôs density function of Z(\\tau), then that is to show the derivative \\partial_{\\tau} f(z, \\tau) is non-negative.\n\n\nThe following claim is from Lebesgue‚Äôs dominated convergence theorem. The differentiation can be swapped with the Laplace transform.\n\n\n\\int_{[0, \\infty)^{n}} e^{-\\bf{\\lambda}\\cdot z} \\partial_{\\tau} f(z, \\tau) dz = \\partial_{\\tau} \\int_{[0,\\infty)^{n}} e^{-\\bf{\\lambda}\\cdot z} f(z, \\tau) dz\nwhich equals to\n \\int_{[0,\\infty)^{n}} e^{-\\bf{\\lambda}\\cdot z} f(z, \\tau) dz = \\mathbb{E} \\exp(-\\frac{1}{2}\\sum_{i=1}^n \\lambda_i X_i^2(\\tau)) = |I + \\Lambda C(\\tau)|^{-1/2}.\n\n\nThe rest is a linear algebra problem only. Here \\Lambda is not important anymore, we drop it as identity.\n\n\n \\det (I + C(\\tau)) = \\det(C_{11} + I)\\det(C_{22} + I - \\tau^2(C_{21})(C_{11} + I)^{-1} (C_{21})^T )\nwhich should be decreasing in \\tau.\nIn the original proof by Thomas Royen, the inequality is extended to the distributions such that the Laplace transform is infinitely divisible.\nüí¨Further discussions\n\nIf the convex sets are not quite symmetric (say up to some local perturbations), does the inequality still hold for Gaussian measure? Such question is raised naturally, some existing works4 5 might be a starting point.\n\nFootnotes\n\n\nSchechtman, Gideon, Th Schlumprecht, and Joel Zinn. ‚ÄúOn the Gaussian measure of the intersection.‚Äù Annals of probability (1998): 346-357. ‚Ü©\n\n\nKhatri, Chinubhai G. ‚ÄúOn certain inequalities for normal distributions and their applications to simultaneous confidence bounds.‚Äù The Annals of Mathematical Statistics (1967): 1853-1867. ‚Ü©\n\n\nKrishnamoorthy, A. S., and M. Parthasarathy. ‚ÄúA multivariate gamma-type distribution.‚Äù The Annals of Mathematical Statistics 22.4 (1951): 549-557. ‚Ü©\n\n\nCordero-Erausquin, Dario. ‚ÄúSome applications of mass transport to Gaussian-type inequalities.‚Äù Archive for rational mechanics and analysis 161 (2002): 257-269. ‚Ü©\n\n\narxiv.org/pdf/1011.4166 ‚Ü©\n\n\n"},"2022-Fall/on-Gr√∂nwall's-inequality":{"title":"on Gr√∂nwall's inequality","links":[],"tags":["math-AP","math-CA"],"content":"üè∑Ô∏èIntroduction\nWe start with a simple application of Gr√∂nwall‚Äôs inequality.\n\n\n                  \n                  Gr√∂nwall&#039;s inequality \n                  \n                \n\nIf the following relation holds\n0\\le f(x) \\le C \\int_0^x f(s) ds,\\quad x\\in [0, A],\nthen f(x) = 0.\n\n\nThe traditional way (of course a nice one) is letting\nh(x) = e^{-C x} \\int_0^x f(s) ds,\nand utilize the relation\nh&#039;(x) = e^{-C x} \\left(f(x) - C \\int_0^x f(s) ds\\right) \\ge 0.\nHere, we attempt with an alternative way.  We define the integral operator \\mathcal{A}: L^{\\infty}_{+}[0, A]\\mapsto L^{\\infty}_{+}[0, A] by the following:\n\\mathcal{A} f:= C\\int_0^x f(s) ds.\nThen \\mathcal{A} is a positive and compact operator (why?).  According to the Krein-Rutman theorem1, if its spectral radius r(\\mathcal{A}) is a positive eigenvalue, and we must have the corresponding eigenfunction \\phi(x) &gt; 0 strictly positive. However, the definition of \\mathcal{A} implies that \\mathcal{A}\\phi (0) = 0, which gives a contradiction. Therefore, the spectral radius r(\\mathcal{A}) = 0.\nObserve that 0\\le f(x) \\le \\mathcal{A} f(x) implies 0\\le f(x) \\le \\mathcal{A}^n f(x) (why?), the Gelfand‚Äôs formula implies \\|\\mathcal{A}^k\\|_{op}\\to 0 as k\\to\\infty, hence f(x) = 0 by taking n\\to \\infty.\n\nüìùExtension\nIf there is another operator \\mathcal{B} that commutes with \\mathcal A, then the spectral radius r(\\mathcal{A} + \\mathcal{B}) can be estimated. We consider the abstract problem as follows.\n\n\n                  \n                  Extension of Gr√∂nwall&#039;s inequality \n                  \n                \n\nLet  \\mathcal{B} be a linear positive operator with r(B) &lt; 1 that commutes with \\mathcal{A}, and\n0\\le f(x) \\le \\mathcal{A} f + \\mathcal{B} f,\nthen f(x) = 0.\n\n\nThe commutativity implies an estimate r(\\mathcal A + \\mathcal{B}) \\le r(\\mathcal{A}) + r(\\mathcal{B}) &lt; 1 (why?). Then the same argument holds since (\\mathcal{A} + \\mathcal{B})^k\\to 0 as k\\to \\infty.\nThis conclusion seems somewhat trivial.  Let us consider an immediate application in transport equation.\nüìñBackground of transport equation\nThe transport equation describes the dynamics of radiative particles interacting with the environment (absorption, scattering, etc.). For instance, supposing the medium is homogeneous, the governing equation can be written in the following form:\n\\begin{aligned}\nv\\cdot \\nabla u(x, v) + \\sigma_a u &amp;= \\sigma_s (\\mathcal{K} u - u)\\quad\\text{ in } D\\times \\mathbb{S}^{d-1},\\\\\nu|_{\\Gamma_{-}} &amp;= h(x, v),\n\\end{aligned}\nwhere \\mathcal{K} is the scattering operator, which represents the probability of scatter events that change direction v&#039; to the direction v.  The function h(x, v) is the source defined on the incoming boundary set:\n\\Gamma_{-} = \\left\\{(x, v)\\in\\partial D\\times \\mathbb{S}^{d-1}\\mid v\\cdot n(x) &lt; 0 \\right\\}. \nüî¶Cone-beam source\nThe cone-beam source function means h(x, v) is quite focusing. The precise definition is the following.\n\n\n                  \n                  Cone-beam source \n                  \n                \n\nIf h(x, v) satisfies that\n\nh is non-negative;\nThere exists a set V\\subset \\mathbb{S}^{d-1} that \\operatorname{supp} h\\subset \\partial D \\times V\\cap \\Gamma_{-}.\n\nThen h is called a cone-beam source.\n\n\nIntuitively, this means the source is one-way dominated (like a laser) and only supported on a certain subset of \\Gamma_{-}. In a special case that \\sigma_s = 0, we find the solution can be solved directly.\nu(x, v) = h(x - \\tau_{-}(x, v)v, v) E(x, x - \\tau_{-}(x, v)v), \\quad E(x, x - sv) = \\exp\\left( - \\int_0^s \\sigma_a(x - tv) dt\\right),\nwhere \\tau_{-}(x, v) denotes the distance from x to the boundary following -v direction.\n„Ä∞Ô∏èNonlinearity\nIn a practical scenario, \\sigma_a may depend on the solution‚Äôs flux (e.g., multi-photon absorption), therefore we obtain a nonlinear equation\nu(x, v) = h(x - \\tau_{-}(x, v) v, v) \\exp\\left( - \\int_0^{\\tau_{-}(x, v)} \\sigma_a(x - tv, \\langle u \\rangle ) dt\\right), \\quad \\langle u \\rangle = \\int_{\\mathbb{S}^{d-1}} u(x, v) dv,\nwhere dv represents the usual probability measure on the sphere. Here, we need some continuity assumption.\n\n\n                  \n                  Assumption on Lipschitz continuity \n                  \n                \n\n\\newcommand{\\aver}[1]{\\langle #1 \\rangle}\n|\\sigma_a(x, \\aver{u}) - \\sigma_a(x, \\aver{w})| \\le L |\\aver{u}(x) - \\aver{w}(x)|\n\n\nLet us first assume the existence of the solution and focus on the uniqueness. If there exists a solution w different from u, then we have:\nu - w = h(x-\\tau_{-}(x, v)v, v) \\left[ \\exp\\left(-\\int_0^{\\tau_{-}(x,v)} \\sigma_a(x-tv, \\langle u \\rangle )dt\\right)-  \\exp\\left(-\\int_0^{\\tau_{-}(x,v)} \\sigma_a(x-tv, \\langle w \\rangle )dt\\right)\\right].\n\n\n                  \n                  Lemma \n                  \n                \n\nIf a, b \\ge c \\ge 0, then |e^{-a} - e^{-b}| \\le e^{-c} |a - b|.\n\n\nLet f:= \\langle u - w\\rangle, using this simple lemma, and integrate over \\mathbb{S}^{d-1}, there exists a constant C &gt; 0 that\n|f(x)| \\le \\overline{h}  \\int_{V} \\int_0^{\\tau_{-}(x,v)} |\\sigma_a(x - tv, \\langle u \\rangle) - \\sigma_a(x - tv, \\langle w \\rangle)| dt dv \\le C   \\int_{V} \\int_0^{\\tau_{-}(x,v)} |f(x-tv)| dt dv,\nwhere \\overline{h} = \\sup_{\\Gamma_{-}} h and C is a constant.  This inequality is just an analog of the previous Gr√∂nwall‚Äôs inequality, which has zero spectral radius. Thus, we must have f(x) = 0, which proves the uniqueness.\n\n\n                  \n                  Remark \n                  \n                \n\nThe general uniqueness can be proved in a different flavor, but it requires slightly more restrictive dependence of \\sigma_a on \\langle {u} \\rangle. For scatter-free medium plus a cone-beam source, it only needs Lipschitz continuity.\n\n\nüåÄIsotropic Scattering\nOnce the scattering is present, the uniqueness is slightly more challenging (there is an alternative way to prove this), assume that \\sigma_s is a positive constant, then we can represent the solution by (let \\sigma_t := \\sigma_a + \\sigma_s)\n\\begin{aligned}\nu(x, v) = &amp;\\,h(x - \\tau_{-}(x, v) v, v) \\exp\\left( - \\int_0^{\\tau_{-}(x, v)} \\sigma_t(x - tv, \\langle u \\rangle ) dt\\right) \\\\&amp;+ \\int_0^{\\tau_{-}(x, v)} \\exp\\left( - \\int_0^{s} \\sigma_t(x - tv, \\langle u \\rangle ) dt\\right) \\sigma_s \\langle u \\rangle(x-sv) ds.\n\\end{aligned}\nSimilar to the previous derivation, we let f: = \\langle u - w\\rangle, then integrate the above equation over the whole \\mathbb{S}^{d-1}, it shows (using the lemma, polar coordinate transformation, maximum principle)\n\\begin{aligned}\n|f(x)| &amp;\\le C   \\int_{V} \\int_0^{\\tau_{-}(x,v)} |f(x-tv)| dt dv + \\frac{1}{\\nu_{d-1}}\\int_{D} \\frac{e^{-\\sigma_s|x-y|}}{|x - y|^{d-1}} \\sigma_s  |f (y)| dy \\\\&amp;\\qquad+ \\int_{\\mathbb{S}^{d-1}} \\int_0^{\\tau_{-}(x, v)} \\int_0^s e^{-s \\sigma_s}L|f(x - tv)| \\sigma_s \\langle u \\rangle(x-sv) dt ds dv  \\\\\n&amp;\\le C   \\int_{V} \\int_0^{\\tau_{-}(x,v)} |f(x-tv)| dt dv + \\frac{1 + L\\ell \\overline{h}}{\\nu_{d-1}}\\int_{D} \\frac{e^{-\\sigma_s|x-y|}}{|x - y|^{d-1}} \\sigma_s  |f (y)| dy,\n\\end{aligned}\nwhere \\ell = \\operatorname{diam}(D). We make the following observation.\n\n\n                  \n                  Commutativity Lemma \n                  \n                \n\nLet \\mathcal{A} and \\mathcal{S} be L^2(D)\\mapsto L^2(D) operators,\n\\mathcal{A} f:= \\int_{V} \\int_0^{\\tau_{-}(x,v)} |f(x-tv)| dt dv,\\quad S f :=\\displaystyle\\int_{D} \\frac{e^{-\\sigma_s |x - y|}}{|x - y|^{d-1}} f(y) dy.\nThen \\mathcal{A} commutes with \\mathcal{S}.\n\n\nA quick proof for this property. Let f be extended to \\mathbb{R}^d with zero outside D, then we can write\nS \\mathcal{A} f = \\int_{\\mathbb{R}^d} \\int_{V}\\int_0^{\\infty} \\frac{e^{-\\sigma_s |x - y|}}{|x - y|^{d-1}} f(y - sv) ds dv dy,\nand we can derive \\mathcal{A}\\mathcal{S} similarly:\n\\mathcal{A} S f = \\int_{V}\\int_0^{\\infty} \\int_{\\mathbb{R}^d} \\frac{e^{-\\sigma_s|x-sv- y|}}{|x-sv-y|^{d-1}} f(y) dy ds dv = \\int_{V} \\int_0^{\\infty} \\int_{\\mathbb{R}^d} \\frac{e^{-\\sigma_s|x-sv - (y&#039;-sv)|}}{|x - y&#039;|^{d-1}} f(y&#039;-sv) dy&#039; ds dv.\nThat implies the following relation.\n|f(x)| \\le C \\mathcal{A} |f| + \\frac{(1 + L\\ell \\overline{h})}{\\nu_{d-1}} \\sigma_s \\mathcal{S} |f|.\nThen we have the concluding theorem by noticing that \\|\\mathcal{S}\\|_{op} \\le 1 - e^{-\\sigma_s \\ell} (why?).\n\n\n                  \n                  Uniqueness Theorem \n                  \n                \n\nIf (1 + L\\ell \\overline{h}) (1 - e^{-\\sigma_s \\ell}) &lt; 1, then the cone-beam source permits a unique solution (if exists).\n\n\nüìîNotes\n\n\nThis uniqueness result simply serves as an exercise utilizing the Gr√∂nwall‚Äôs inequality. This result is only feasible for weak scattering medium.\n\n\nThe commutativity is necessary to estimate the spectral radius, a slightly more general condition is mentioned in2.\n\n\nFootnotes\n\n\nen.wikipedia.org/wiki/Krein%E2%80%93Rutman_theorem ‚Ü©\n\n\nZima, M. ‚ÄúA theorem on the spectral radius of the sum of two operators and its application.‚Äù Bulletin of the Australian Mathematical Society 48.3 (1993): 427-434. ‚Ü©\n\n\n"},"2022-Fall/on-Sinkhorn's-Theorem":{"title":"on Sinkhorn's Theorem","links":[],"tags":["math-NA"],"content":"Background\nSinkhorn‚Äôs Theorem bridges the positive matrices (positive entries) with the doubly stochastic matrices by diagonal multipliers, i.e., there are diagonal matrices D_1 and D_2 that\nD_1 A D_2 e = e, \\quad e^T D_1 A D_2 = e^T\nThe proof can be found in various methods, but most are based on certain kind of fixed-point property. For instance, the geometric proof uses the Brouwer‚Äôs fixed-point theorem. The convergence of the Sinkhorn‚Äôs algorithm seeks for the vectors x and y by iteratively computing\ny_{k+1} = r./(A x_k), \\quad x_{k+1} = c ./ (A^T y_{k+1}) \nwhere r is the row sum vector and c is the column sum vector. They can be merged into one iteration by\nx_{k+1} = T (x_k):= c ./ (A^T (r ./ A x_{k})).\nThe theory uses the nonlinear Perron-Frobenius theorem to find the fixed-point. Clearly, the map T sends the positive cone \\mathbb{R}^{n}_+ to itself, here we need a little bit more compactness to exploit the fixed-point theorem. Either seeking for the boundedness or define \\widehat{T} x = T(x) / \\|T(x)\\|_1, then a fixed-point of \\widehat{T} also works due to invariance of scaling. When the matrix A is relaxed to only nonnegative entries, there are proofs showing if A is fully indecomposable, then the same conclusion holds.\nThoughts\nBy a simple generalization, it is natural to ask for non-singular integral kernel A(x,y) &gt; 0 such that\n\\int_{D_q} p(x) A(x, y) q(y) d y = 1,\\quad \\int_{D_p} p(x) A(x, y) q(y) d x = 1\nThen a natural iteration will be h_{k+1}(y) = T h_k:= \\dfrac{c(y)} { \\displaystyle\\int_{D_q} \\dfrac{ A(x, y) r(x) }{ \\displaystyle\\int_{D_p} A(x,y) h_k(y) dy }dx} \nand we seek for a fixed-point of h = \\widehat{T} h, where \\widehat{T} is a normalization of T, the boundedness of h_k comes at no price, for a certain kind of compactness, we can add the equi-continuity requirement which becomes a smoothness condition on A(x,y) such that A(x, y) is equi-continuous in y, then a convergent subsequence of h_k will be what we need. Therefore A can be also a weakly singular kernel as well.\nIf A(x, y, t) is an evolution of integral operator converges as t\\to\\infty which gradually looses the equi-continuity or boundedness, it is interesting to see whether the corresponding solution h^{\\ast}(y, t) also converges.\nNotes\n\n\n\nLinks\n\n\n"},"2022-Fall/on-epsilon-extrapolation":{"title":"on epsilon extrapolation","links":[],"tags":["math-NA"],"content":"Overview\nNotes\n\n\n\nLinks\n\n\n"},"2022-Fall/on-models-of-chemotaxis":{"title":"on models of chemotaxis","links":[],"tags":["math-AP"],"content":"Introduction\nThoughts\nNotes\n\n\n\nLinks\n\n\n"},"2022-Fall/on-two-layer-ReLU-networks":{"title":"on two-layer ReLU networks","links":["2025-Summer/on-subspace-spanned-by-trajectory","2023-Spring/on-eigenvalue-estimate-for-sample-matrix"],"tags":["math-NA","math-CA","math-AP"],"content":"\n\n                  \n                  Info\n                  \n                \n\nThis note is not intended to be a literature review of any kind nor an original research piece, it is just for fun.\n\n\nüè∑Ô∏èIntroduction\nThe ReLU (rectified linear unit) refers to the following operation\n\\operatorname{ReLU}(x) = \\max(0, x).\nLet us denote this function by \\sigma for short. Trivially, we find that\n\n\\sigma&#039;&#039;(x) = \\delta(x) which makes \\sigma a certain Green‚Äôs function (w.r.t some boundary conditions) for one dimensional Laplacian.\n\\sigma(c x) = c\\sigma(x) for any positive real number c.\n\nIn higher dimensions, the ReLU function is often used with a linear layer:\ny = \\sigma(w\\cdot x - b),\nwhere the vector/matrix w is the weights, and the scalar/vector b is the bias. The commutativity with scalar multiplication makes it possible to consider only normalized weights or normalized input x.\n\n\n                  \n                  Definition \n                  \n                \n\nTwo layer ReLU network is defined by\nf(x) = \\int_{V} h(w, b) \\sigma(w\\cdot x - b) d\\mu(w, b),\nwhere \\mu is the usual measure on the joint space V = \\mathbb{S}^{n-1}\\times \\mathbb{R}.\n\n\n‚òòÔ∏èRadon transform\nFor some input x\\in\\mathbb{R}^n, if we let w\\in \\mathbb{S}^{n-1}, then\n\\nabla \\sigma(w\\cdot x - b) = w H(w\\cdot x - b),\nwhere H is the Heaviside function and \\Delta \\sigma(w\\cdot x - b) = \\nabla \\cdot \\nabla  \\sigma(w\\cdot x - b) = w\\cdot w \\delta(w\\cdot x - b) = \\delta(w\\cdot x - b). Therefore, (at least formally or in weak sense)\n\\Delta f(x) = \\int_V h(w, b) \\delta(w\\cdot x - b) d\\mu(w, b) = \\int_{\\mathbb{S}^{n-1}} h(w, w\\cdot x) d\\mu(w) = \\mathcal{R}^{\\ast} h,\nwhere \\mathcal{R}^{\\ast} is the adjoint Radon transform and h is a distribution in the dual space. The intertwining property and the inversion formula are well-known.\n\n\n                  \n                  Intertwining Property \n                  \n                \n\n\\mathcal{R}\\Delta f = \\partial_b^2 \\mathcal{R} f,\\quad \\mathcal{R}^{\\ast} \\partial_b^2 g = \\Delta \\mathcal{R}^{\\ast} g.\n\n\n\n\n                  \n                  Helgason&#039;s Inversion Formula \n                  \n                \n\nIn Fourier sense, the inversion formula holds for f\\in C^{\\infty}(\\mathbb{R}^n) with decay |f(x)| = O(|x|^{-N}) for some N &gt; n:\n(-\\Delta)^{\\frac{n-1}{2}}\\mathcal{R}^{\\ast}\\mathcal{R} f = c_n f \n\n\nWhen appropriate, we can write f(x) = \\Delta^{-1}\\mathcal{R}^{\\ast} h in Fourier sense, where \\Delta^{-1} is a Fourier multiplier, and the singular values of the operator \\Delta^{-1}\\mathcal{R}^{\\ast} is essential in terms of L^2 optimization.\n\n\n                  \n                  Observation \n                  \n                \n\nIn Fourier sense\n\\Delta^{-1}\\mathcal{R}^{\\ast}\\mathcal{R}\\Delta^{-1}= \\frac{1}{c_n}(-\\Delta)^{-\\frac{n+3}{2}}.\n\n\nBy Weyl‚Äôs law, it implies that the singular values of the operator \\Delta^{-1}\\mathcal{R}^{\\ast} will be decaying like \\sigma_k = \\Theta(k^{-(n+3)/n}).  It explains the low-pass filtration property of two-layer ReLU network.\nüî≠Spectral perspective\nThere are two common (easy yet boring) configurations under the scope of spectral perspectives.\nüß©Neural-Tangent-Kernel (NTK)\nThe NTK configuration assumes sufficiently wide network, which makes the training behavior degenerated to a linear ODE (under gradient flow).\n\\frac{d\\mathcal{R}^{\\ast}h}{dt} = -(-\\Delta)^{-\\frac{n+3}{2}} \\mathcal{R}^{\\ast} h, \nAt a first sight, we find this evolution equation will be sustaining the initial ‚Äúnoises‚Äù for a long time. It also infers that a highly sophisticated initialization (that contains high frequencies) could be toxic to the training process if those are not desired. Interestingly, a similar formulation is also seen in on subspace spanned by trajectory, but from a different perspective.\nüß©Random Feature (RF)\nThe RF configuration can be viewed as a discrete version of NTK, but the width of network does not have to be wide. The discrepancy between the eigenvalues in continuous and discrete cases can be studied through trivial tools like Weyl‚Äôs inequality or slightly more complex ways. It is not the focus of this post, we will leave this topic to a later post, see on eigenvalue estimate for sample matrix.\nüí°Training dynamics\nA central topic around neural networks is the quantification of generalization (statistical, approximation, training) error. Mathematically, the only challenging term is to bound the training error, which demands a comprehensive exploration of the training dynamics.\nüåµ Preliminaries\nWe consider a generic formulation in one dimension:\nf(x, t) = \\sum_{i=1}^n a_i(t) \\sigma(x - b_i(t))\nThe biases b_i(0) are initialized on [-1, 1] uniformly (or equispaced), the weight‚Äôs initialization will be discussed later.  Suppose the target function is denoted by g, and we naively consider the gradient flow.\nwe introduce the auxiliary function w(b, t) that\n\\partial^2_b w(b, t) = f(b, t) - g(b). \nThen, taking account of the ReLU function, it is quite easy to derive:\n\n\n                  \n                  Observation \n                  \n                \n\n\\partial_b^4 w(b, t) = \\sum_{k=1}^{\\infty} \\Theta_k(t) \\phi_k(b)\nwhere \\{ \\phi_k \\}_{k\\ge 1}  are the eigenfunctions of the following problem:\n\\phi_k^{(4)} = \\lambda_k \\phi_k,\\qquad \\phi_k(1) = \\phi_k&#039;(1) = \\phi_k&#039;&#039;(-1) = \\phi_k&#039;&#039;&#039;(-1) = 0.\n\n\n\nüí¨ Further discussion\nüåä Initialization\nNotes\n\n\n\nLinks\n\non eigenvalue estimate for sample matrix\non subspace spanned by trajectory\narxiv.org/pdf/2306.17301\n"},"2023-Spring/on-1D-Ising-models":{"title":"on 1D Ising models","links":[],"tags":["math-CA","math-MP"],"content":"Notes\n\n\n\nLinks\n\n\n"},"2023-Spring/on-eigenvalue-estimate-for-sample-matrix":{"title":"on eigenvalue estimate for sample matrix","links":["2023-Summer/on-convergence-of-graph-Laplacian's-eigenvalues","2022-Fall/on-two-layer-ReLU-networks"],"tags":["math-CA","math-PR"],"content":"Introduction\nGiven a positive definite kernel function \\mathcal{G}(x, y), it is natural to sample some points (x_i, x_j) (so it creates a matrix G) as an approximation to the kernel function. It is a classical topic to estimate the eigenvalues of G and compare with \\mathcal{G}‚Äòs eigenvalues.\nThe simplest idea to quantify the difference is probably Weyl‚Äôs inequality for self-adjoint compact operators.\n\n\n                  \n                  Estimate by Weyl&#039;s inequality \n                  \n                \n\nLet \\Omega=[0, 1]^n be a unit cube and \\mathcal{G}(x, y)\\in C^1(\\Omega\\times \\Omega) is a positive definite kernel. The matrix G\\in \\mathbb{R}^{m\\times m} has entries as \\mathcal{G}(x_i, x_j) for \\{x_i\\}_{i=1}^m be the regular ‚Äúlattice points‚Äù with \\sqrt[n]{m} points along each axis. Then\n|\\mu_i(\\mathcal{G}) - \\frac{1}{m}\\mu_i(G)| = O\\left(\\frac{1}{\\sqrt[n]{m}}\\right)\nwhere \\mu_i denotes the ith eigenvalue in descending order.\n\n\n\n\n                  \n                  Proof \n                  \n                \n\nThe lattice points automatically generate a partition of \\Omega, we denote C(x_i) as the small cube centered at x_i which is disjoint from other small cubes.\nThe matrix G constructs an approximated kernel function on each small cube around the lattice points. We define \\mathcal{G}^{\\ast}(x, y) as\n\\mathcal{G}^{\\ast}(x, y) = \\mathcal{G}(x_i, y_j)\\quad (x, y)\\in C(x_i)\\times C(y_j).\nIt is straightforward to see \\mu_i(\\mathcal{G}^{\\ast}) = \\frac{1}{m}\\mu_i(G).  Then by Weyl‚Äôs inequality,\n\\begin{aligned}\n|\\mu_i(\\mathcal{G}) - \\mu_i(\\mathcal{G}^{\\ast})| &amp;\\le \\|\\mathcal{G} - \\mathcal{G}^{\\ast}\\|_{op} \\le \\sqrt{\\int_{\\Omega\\times\\Omega} |\\mathcal{G}(x, y) - \\mathcal{G}^{\\ast}(x, y)|^2 dx dy} \\\\\n&amp;= \\sqrt{\\sum_{i=1}^m\\sum_{j=1}^m \\int_{C(x_i)\\times C(x_j)} |\\mathcal{G}(x, y) - \\mathcal{G}^{\\ast}(x, y)|^2 dx dy} \\\\\n&amp;= O \\left( \\frac{\\|\\nabla \\mathcal{G}\\|_{\\infty}}{\\sqrt[n]{m}} \\right). \n\\end{aligned}\n\n\nIntuitively, the insufficiency of samples will only affect smaller eigenvalues (more oscillatory eigenfunctions), thus isolating the ‚Äúsafer‚Äù eigenfunctions is natural.\nLet the kernel \\mathcal{G} be expanded into its eigenfunctions:\n\\mathcal{G}(x, y) = \\sum_{k=1}^{\\infty} \\mu_k(\\mathcal{G}) \\psi_k(x) \\psi_k(y).\nAnd we pick a truncation \\mathcal{G}_l:= \\sum_{k=1}^{l} \\lambda_k \\psi_k(x) \\psi_k(y). Then the matrix G can be split into two parts:\nG_{i, j} = \\mathcal{G}_l(x_i, x_j) + (\\mathcal{G}(x_i, x_j) - \\mathcal{G}_l(x_i, x_j)).\nThe first part should be less affected by the sampling, thus we should expect the correlation\n\\frac{1}{m}\\sum_{i=1}^m \\psi_{k}(x_i) \\phi_r (x_i)\\approx \\delta_{k, r}\nThe common tool we use is Ostrowski‚Äôs theorem1.\n\n\n                  \n                  Ostrowski theorem \n                  \n                \n\nIf A\\in \\mathbb{C}^{n\\times n} and X\\in\\mathbb{C}^{n\\times n}, then\n\\mu_i(X^{\\ast}A X) = \\theta_i \\mu_i(A), i\\in [n].\nwhere \\mu_n(X^{\\ast} X) \\le \\theta_i \\le \\mu_1(X^{\\ast}X).\n\n\nLet (\\Phi_l)_{i,j} = \\psi_{j}(x_i) and \\Lambda_l = \\operatorname{diag}(\\mu_i(\\mathcal{G}))_{i=1}^l, then the theorem implies\n\\left|\\frac{1}{m}\\mu_i(\\Phi_l\\Lambda_l \\Phi_l^{\\ast}) - \\mu_i(\\mathcal{G})\\right|\\le \\mu_i(\\mathcal{G})\\left\\|\\frac{1}{m}\\Phi_l^T\\Phi_l - Id_l\\right\\|_{op}.\nThen, we obtain the bound (Ostrowski theorem and Weyl‚Äôs theorem)\n\\begin{aligned}\n|\\mu_i(\\mathcal{G}) - \\frac{1}{m}\\mu_i(G)| &amp;\\le |\\mu_i(\\mathcal{G}) -\\frac{1}{m} \\mu_i(\\Phi_l\\Lambda_l\\Phi_l^T)| + |\\frac{1}{m}\\mu_i(\\Phi_l\\Lambda_l\\Phi_l^T) - \\frac{1}{m}\\mu_i(G)| \\\\\n&amp;\\le \\mu_i(\\mathcal{G}) \\left\\|\\frac{1}{m}\\Phi_l^T\\Phi_l - Id_l\\right\\|_{op} + \\frac{1}{m}\\|G - \\Phi_l\\Lambda_l \\Phi_l^{\\ast}\\|_{op}.\n\\end{aligned}\n\nThe remainder term G - \\Phi_l\\Lambda_l \\Phi_l^{\\ast} = \\sum_{k &gt; l} \\mu_k \\psi_k(x_i)\\psi_k(x_j), a naive bound of the 2nd term will be C \\sum_{k &gt; l} \\mu_k if the eigenfunctions are uniformly bounded, otherwise the growth needs to be considered.\nThe first term can be quantified by viewing x_i as a certain quadrature rule, or ready for Hoeffding‚Äôs inequality. We should expect the stochastic bound O(m^{-1/2}\\sqrt{\\log l^2/p}) for each entry with probability 1 - \\frac{p}{l^2}, therefore, with probability 1 - p,\n \\left\\|\\frac{1}{m}\\Phi_l^T\\Phi_l - Id_l\\right\\|_{op} = O\\left(\\sqrt{m \\log l^2/p}\\right).\n\n\nAnother approach for well-structured points x_i is using the min-max principle for eigenvalues, which usually involves an explicit construction of subspace, see also on convergence of graph Laplacian‚Äôs eigenvalues.\nLinks\n\non two-layer ReLU networks\non convergence of graph Laplacian‚Äôs eigenvalues\n\nFootnotes\n\n\nen.wikipedia.org/wiki/Ostrowski%27s_theorem ‚Ü©\n\n\n"},"2023-Summer/on-convergence-of-graph-Laplacian's-eigenvalues":{"title":"on convergence of graph Laplacian's eigenvalues","links":[],"tags":["math-NA","math-CA","math-AP"],"content":"\n\n                  \n                  Info\n                  \n                \n\nThis post is more like an exercise for personal interest. It does not necessarily contain anything new.\n\n\nüè∑Ô∏èIntroduction\nLet us imagine that a closed smooth manifold is constructed in minecraft üòÖ, we can expect some detailed information is likely to lose. So, it is natural to ask:\n\nHow much geometric information is still kept?\n\nLet \\Gamma\\subset \\mathbb{R}^3 be a smooth, closed, oriented surface. And \\Delta_{\\Gamma} is the Laplace-Beltrami operator on \\Gamma and \\nabla_{\\Gamma} be the gradient operator. We denote \\{\\phi_k\\}_{k\\ge 0} as the eigenfunctions of -\\Delta_{\\Gamma} with corresponding eigenvalues \\{\\lambda_k\\}_{k\\ge 0}. The first trivial eigenvalue \\lambda_0 = 0.\nüåµEigenvalue estimates in tube\nLet d_{\\Gamma} denote the signed distance function to \\Gamma that takes the negative sign for points inside the region enclosed by \\Gamma. With the distance function to \\Gamma, the projection operator can be evaluated by\n    P_{\\Gamma}(x) := x - d_{\\Gamma}(x) \\nabla d_{\\Gamma}(x).\nThe projection operator is well-defined when the |d_{\\Gamma}(x)|^{-1} is larger than the maximum principal curvature of \\Gamma.\n\n\n                  \n                  Definition \n                  \n                \n\nLet \\Gamma_t be the level set\n\\Gamma_t:= \\{x\\in \\mathbb{R}^3 \\mid d_{\\Gamma} x = t\\}\nand the tube D_t:= \\{x\\in \\mathbb{R}^3 \\mid |d_{\\Gamma} x| &lt; t\\}\n\n\nThen we prove the following lemma, which shows the eigenvalues of the Laplacian on D_r can approximate the eigenvalues of \\Delta_{\\Gamma} as the tube width r\\ll 1.\n\n\n                  \n                  Lemma \n                  \n                \n\nLet r &gt; 0 be a small parameter. Assume that \\psi_0, \\psi_1, \\cdots are the eigenfunctions of the following eigenvalue problem with Neumann boundary condition\n\\begin{aligned}\n-\\Delta \\psi_k &amp;= \\mu_k \\psi_k &amp; \\text{ in }&amp; D_r\\\\\n\\frac{\\partial \\psi_k}{\\partial n} &amp;= 0 &amp;\\text{ on }&amp;\\partial D_r.\n\\end{aligned} \nThen there exists a constant C &gt; 0 independent of r that |\\lambda_k - \\mu_k| \\le Cr\\lambda_k.\n\n\n\n\n                  \n                  Proof of lower bound \n                  \n                \n\nThe main tool is the min-max principle,\n\\mu_k = \\min_{\\dim U = k+1}\\max_{f\\in U - \\{0\\}} \\frac{\\int_{D_r} |\\nabla f(x)|^2 dx }{\\int_{D_r} |f(x)|^2 dx}.\nWe first provide a lower bound for \\mu_k.  Our estimate consists of three steps.\nStep 1: Explicit construction of the subspace U.  We define the functions \\{h_m\\}_{m=0}^k by\nh_m(x) = \\phi_m(P_{\\Gamma} x).\nand denote U = \\operatorname{span}(h_0,\\cdots, h_k).\nStep 2: Estimate of norms.  Let f\\in U, and we apply the co-area formula\n\\begin{aligned}\n\\int_{D_r} |f(x)|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|f(P_{\\Gamma} x)|^2 dS d t\n\\end{aligned}\nLet J(x) be the Jacobian between the surfaces \\{d_{\\Gamma} = t\\} and \\Gamma, then\n\\int_{d_{\\Gamma}(x) = t}|f(P_{\\Gamma} x)|^2 J(x) dS = \\int_{\\Gamma} |f(x)|^2 dS\nwhere J(x) = 1 - d_{\\Gamma}(x) \\Delta d_{\\Gamma}(x) + |d_{\\Gamma}(x)|^2 \\langle{\\nabla d_{\\Gamma}, \\nabla^2 d_{\\Gamma} \\nabla d_{\\Gamma}}\\rangle.\nTherefore,  J(x) = 1 + O(r), we have the following estimate\n\\int_{D_r} |f(x)|^2 dx = 2r \\left(1 + O(r)\\right)  \\int_{\\Gamma} |f(x)|^2 dS.\nWe then estimate\n\\begin{aligned}\n\\int_{D_r} |\\nabla f|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(P_{\\Gamma} x)|^2 dS d t = 2r(1 + O(r)) \\int_{\\Gamma} |\\nabla f(x)|^2 dS \\\\\n&amp;= 2r(1 + O(r)) \\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS.\n\\end{aligned}\nStep 3: Apply the min-max principle.\n\\begin{aligned}\n\\mu_k &amp;\\le (1 + O(r)) \\max_{f\\in U -\\{0\\}} \\frac{\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS} \\\\\n&amp;= (1 + O(r)) \\lambda_k.\n\\end{aligned}\n\n\n\n\n                  \n                  Proof of upper bound \n                  \n                \n\nNext, we provide an upper bound estimate using the max-min counterpart.\n\\mu_k = \\max_{\\dim U = k} \\min_{f \\in U^{\\perp} - \\{0\\}}   \\frac{\\int_{D_r} |\\nabla f(x)|^2 dx }{\\int_{D_r} |f(x)|^2 dx}.\nStep 4: Explicit construction of U.  We define the functions \\{w_m\\}_{m=0}^{k-1} by\nw_m(x) = \\sqrt{\\mathcal{J}(x)} \\phi_m(P_{\\Gamma}x) ,\\quad {\\mathcal{J}}(x):=\\frac{J(x)+J(2P_{\\Gamma} x - x)}{2}.\nwhere 2P_{\\Gamma} x - x is the mirror point of x across \\Gamma. Then for \\forall i\\neq j, using the symmetry of the domain along normal direction,\n\\begin{aligned}\n\\int_{D_r} w_i(x) w_j(x) dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} w_i(x) w_j(x) dS d t   \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) \\mathcal{J}(x) dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) J(x) dS d t\\\\\n&amp;= 2r \\int_{\\Gamma} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) dS = 0.\n\\end{aligned}\nwhich implies \\{w_m\\}_{m=0}^{k-1} forms an orthogonal set and let U = \\text{span}(w_0,\\cdots, w_{k-1}).\nStep 5: Norm estimates are similar to the previous case. For f\\in U, we can write f(x) = \\sqrt{\\mathcal{J}(x)} \\tilde{f}(x) that \\tilde{f}\\in \\text{span}(h_0, \\cdots, h_m) and\n\\int_{d_{\\Gamma}(x) = t} |f(x)|^2 dS = \\int_{\\Gamma} |\\tilde{f}(x)|^2 dS.\nwhich implies\n\\int_{D_r} |f(x)|^2 dx = 2r  \\int_{\\Gamma} |\\tilde f(x)|^2 dS.\nIn a similar spirit, we have\n\\begin{aligned}\n\\int_{D_r} |\\nabla f|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}\\left|\\sqrt{\\mathcal{J}(x)}\\nabla \\tilde{f}(P_{\\Gamma} x) + \\tilde{f}(x) \\nabla \\sqrt{\\mathcal{J}(x)} \\right|^2 dS d t.\n\\end{aligned}\nBy cancellation, \\mathcal{J}(x) = 1 + O(r^2), which implies that \\sqrt{\\mathcal{J}(x)} = 1 + O(r^2) and |\\nabla \\sqrt{\\mathcal{J}(x)}| = O(r) as well. Therefore,\n\\int_{D_r} |\\nabla f|^2 dx =  2r\\left( (1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS + O(r) \\int_{\\Gamma} |f|^2 dS\\right).\nStep 6: We will now use the max-min principle\n\\begin{aligned}\n\\mu_k &amp;\\ge \\min_{f\\in U^{\\perp}-\\{0\\}} \\frac{(1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS + O(r) \\int_{\\Gamma} |f(x)|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS}\n\\end{aligned}\nFor k\\ge 1, since \\phi_0 is a constant function, we have \\int_{\\Gamma} f(x) dS = 0, then by the Poincar√© inequality, there exists a constant C &#039; &gt; 0 that\n\\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS \\ge C&#039; \\int_{\\Gamma} |f|^2 dS.\nThus we have\n\\begin{aligned}\n\\mu_k &amp;\\ge \\min_{f\\in U^{\\perp} - \\{0\\}} \\frac{(1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS + O(r) \\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS} \\\\&amp;= (1 + O(r)) \\lambda_k.\n\\end{aligned}\n\n\nNotes\n\n\n\nLinks\n\n\n"},"2025-Summer/on-Hardy-inequalities":{"title":"on Hardy inequalities","links":[],"tags":["math-AP"],"content":"Notes\n\n\n\nLinks\n\n\n"},"2025-Summer/on-Weyl‚Äôs-asymptotic-law":{"title":"on Weyl‚Äôs asymptotic law","links":[],"tags":["math-AP","math-MP"],"content":"Motivation\nIntroduction\nThoughts\nConnections\nNotes\n\n\n\nLinks\n\n\n"},"2025-Summer/on-diffusion-models":{"title":"on diffusion models","links":[],"tags":["math-NA","cs-LG"],"content":"Notes\n\n\n\nLinks\n\n\n"},"2025-Summer/on-interpolation-theorems":{"title":"on interpolation theorems","links":[],"tags":["math-AP"],"content":"Notes\n\n\n\nLinks\n\n\n"},"2025-Summer/on-subspace-spanned-by-trajectory":{"title":"on subspace spanned by trajectory","links":[],"tags":["math-AP","math-NA"],"content":"Notes\n\n\n\nLinks\n\n\n"},"index":{"title":"Welcome to Latent Seminar","links":[],"tags":[],"content":"\n\nThis latent seminar is purely for personal interests, and it is inspired by John Baez‚Äôs weekly findings and Terry Tao‚Äôs blog, which I followed since I was an undergraduate student.¬† I found it fascinating to learn from a wide variety of topics.¬† I used to keep a research blog (updated to 2020) to document some trivial thoughts, but unfortunately paused during COVID-19.¬†¬†Now, it is resumed in a different form.\nThe topics are quite random but enjoyable!\n\nüìä Tag Distribution\n---\nconfig:\n    theme: &quot;base&quot;\n    themeVariables:\n      primaryColor: &#039;#81c8be&#039;\n      secondaryColor: &#039;#e5c890&#039;\n      tertiaryColor: &#039;#8caaee&#039;\n---\npie\n    title \n    &quot;math-PR&quot; : 1\n    &quot;math-CA&quot; : 3\n    &quot;math-CO&quot; : 1\n    &quot;math-NA&quot; : 4\n    &quot;math-AP&quot; : 6\n    &quot;math-MP&quot; : 1\n    &quot;cs-LG&quot; : 1\n"},"2023-Summer/on-convergence-of-graph-Laplacian":{"title":"on convergence of graph Laplacian","links":[],"tags":["math-NA","math-CA","math-AP"],"content":"\n\n                  \n                  Info\n                  \n                \n\nThis post is more like an exercise for personal interest. It does not necessarily contain anything new.\n\n\nüè∑Ô∏èIntroduction\nLet us imagine that a closed smooth manifold is constructed in minecraft üòÖ, we can expect some detailed information is likely to lose. So, it is natural to ask:\n\nHow much geometric information is still kept?\n\nLet \\Gamma\\subset \\mathbb{R}^3 be a smooth, closed, oriented surface. And \\Delta_{\\Gamma} is the Laplace-Beltrami operator on \\Gamma and \\nabla_{\\Gamma} be the gradient operator. We denote \\{\\phi_k\\}_{k\\ge 0} as the eigenfunctions of -\\Delta_{\\Gamma} with corresponding eigenvalues \\{\\lambda_k\\}_{k\\ge 0}. The first trivial eigenvalue \\lambda_0 = 0.\nüåµEigenvalue estimates in tube\nLet d_{\\Gamma} denote the signed distance function to \\Gamma that takes the negative sign for points inside the region enclosed by \\Gamma. With the distance function to \\Gamma, the projection operator can be evaluated by\n    P_{\\Gamma}(x) := x - d_{\\Gamma}(x) \\nabla d_{\\Gamma}(x).\nThe projection operator is well-defined when the |d_{\\Gamma}(x)|^{-1} is larger than the maximum principal curvature of \\Gamma.\n\n\n                  \n                  Definition \n                  \n                \n\nLet \\Gamma_t be the level set\n\\Gamma_t:= \\{x\\in \\mathbb{R}^3 \\mid d_{\\Gamma} x = t\\}\nand the tube D_t:= \\{x\\in \\mathbb{R}^3 \\mid |d_{\\Gamma} x| &lt; t\\}\n\n\nThen we prove the following lemma, which shows the eigenvalues of the Laplacian on D_r can approximate the eigenvalues of \\Delta_{\\Gamma} as the tube width r\\ll 1.\n\n\n                  \n                  Lemma \n                  \n                \n\nLet r &gt; 0 be a small parameter. Assume that \\psi_0, \\psi_1, \\cdots are the eigenfunctions of the following eigenvalue problem with Neumann boundary condition\n\\begin{aligned}\n-\\Delta \\psi_k &amp;= \\mu_k \\psi_k &amp; \\text{ in }&amp; D_r\\\\\n\\frac{\\partial \\psi_k}{\\partial n} &amp;= 0 &amp;\\text{ on }&amp;\\partial D_r.\n\\end{aligned} \nThen there exists a constant C &gt; 0 independent of r that |\\lambda_k - \\mu_k| \\le Cr\\lambda_k.\n\n\n\n\n                  \n                  Proof of lower bound \n                  \n                \n\nThe main tool is the min-max principle,\n\\mu_k = \\min_{\\dim U = k+1}\\max_{f\\in U - \\{0\\}} \\frac{\\int_{D_r} |\\nabla f(x)|^2 dx }{\\int_{D_r} |f(x)|^2 dx}.\nWe first provide a lower bound for \\mu_k.  Our estimate consists of three steps.\nStep 1: Explicit construction of the subspace U.  We define the functions \\{h_m\\}_{m=0}^k by\nh_m(x) = \\phi_m(P_{\\Gamma} x).\nand denote U = \\operatorname{span}(h_0,\\cdots, h_k).\nStep 2: Estimate of norms.  Let f\\in U, and we apply the co-area formula\n\\begin{aligned}\n\\int_{D_r} |f(x)|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|f(P_{\\Gamma} x)|^2 dS d t\n\\end{aligned}\nLet J(x) be the Jacobian between the surfaces \\{d_{\\Gamma} = t\\} and \\Gamma, then\n\\int_{d_{\\Gamma}(x) = t}|f(P_{\\Gamma} x)|^2 J(x) dS = \\int_{\\Gamma} |f(x)|^2 dS\nwhere J(x) = 1 - d_{\\Gamma}(x) \\Delta d_{\\Gamma}(x) + |d_{\\Gamma}(x)|^2 \\langle{\\nabla d_{\\Gamma}, \\nabla^2 d_{\\Gamma} \\nabla d_{\\Gamma}}\\rangle.\nTherefore,  J(x) = 1 + O(r), we have the following estimate\n\\int_{D_r} |f(x)|^2 dx = 2r \\left(1 + O(r)\\right)  \\int_{\\Gamma} |f(x)|^2 dS.\nWe then estimate\n\\begin{aligned}\n\\int_{D_r} |\\nabla f|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(P_{\\Gamma} x)|^2 dS d t = 2r(1 + O(r)) \\int_{\\Gamma} |\\nabla f(x)|^2 dS \\\\\n&amp;= 2r(1 + O(r)) \\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS.\n\\end{aligned}\nStep 3: Apply the min-max principle.\n\\begin{aligned}\n\\mu_k &amp;\\le (1 + O(r)) \\max_{f\\in U -\\{0\\}} \\frac{\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS} \\\\\n&amp;= (1 + O(r)) \\lambda_k.\n\\end{aligned}\n\n\n\n\n                  \n                  Proof of upper bound \n                  \n                \n\nNext, we provide an upper bound estimate using the max-min counterpart.\n\\mu_k = \\max_{\\dim U = k} \\min_{f \\in U^{\\perp} - \\{0\\}}   \\frac{\\int_{D_r} |\\nabla f(x)|^2 dx }{\\int_{D_r} |f(x)|^2 dx}.\nStep 4: Explicit construction of U.  We define the functions \\{w_m\\}_{m=0}^{k-1} by\nw_m(x) = \\sqrt{\\mathcal{J}(x)} \\phi_m(P_{\\Gamma}x) ,\\quad {\\mathcal{J}}(x):=\\frac{J(x)+J(2P_{\\Gamma} x - x)}{2}.\nwhere 2P_{\\Gamma} x - x is the mirror point of x across \\Gamma. Then for \\forall i\\neq j, using the symmetry of the domain along normal direction,\n\\begin{aligned}\n\\int_{D_r} w_i(x) w_j(x) dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} w_i(x) w_j(x) dS d t   \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) \\mathcal{J}(x) dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) J(x) dS d t\\\\\n&amp;= 2r \\int_{\\Gamma} \\phi_i(P_{\\Gamma} x) \\phi_j(P_{\\Gamma} x) dS = 0.\n\\end{aligned}\nwhich implies \\{w_m\\}_{m=0}^{k-1} forms an orthogonal set and let U = \\text{span}(w_0,\\cdots, w_{k-1}).\nStep 5: Norm estimates are similar to the previous case. For f\\in U, we can write f(x) = \\sqrt{\\mathcal{J}(x)} \\tilde{f}(x) that \\tilde{f}\\in \\text{span}(h_0, \\cdots, h_m) and\n\\int_{d_{\\Gamma}(x) = t} |f(x)|^2 dS = \\int_{\\Gamma} |\\tilde{f}(x)|^2 dS.\nwhich implies\n\\int_{D_r} |f(x)|^2 dx = 2r  \\int_{\\Gamma} |\\tilde f(x)|^2 dS.\nIn a similar spirit, we have\n\\begin{aligned}\n\\int_{D_r} |\\nabla f|^2 dx &amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}|\\nabla f(x)|^2 dS d t \\\\\n&amp;= \\int_{-r}^r \\int_{d_{\\Gamma}(x) = t}\\left|\\sqrt{\\mathcal{J}(x)}\\nabla \\tilde{f}(P_{\\Gamma} x) + \\tilde{f}(x) \\nabla \\sqrt{\\mathcal{J}(x)} \\right|^2 dS d t.\n\\end{aligned}\nBy cancellation, \\mathcal{J}(x) = 1 + O(r^2), which implies that \\sqrt{\\mathcal{J}(x)} = 1 + O(r^2) and |\\nabla \\sqrt{\\mathcal{J}(x)}| = O(r) as well. Therefore,\n\\int_{D_r} |\\nabla f|^2 dx =  2r\\left( (1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS + O(r) \\int_{\\Gamma} |f|^2 dS\\right).\nStep 6: We will now use the max-min principle\n\\begin{aligned}\n\\mu_k &amp;\\ge \\min_{f\\in U^{\\perp}-\\{0\\}} \\frac{(1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS + O(r) \\int_{\\Gamma} |f(x)|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS}\n\\end{aligned}\nFor k\\ge 1, since \\phi_0 is a constant function, we have \\int_{\\Gamma} f(x) dS = 0, then by the Poincar√© inequality, there exists a constant C &#039; &gt; 0 that\n\\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS \\ge C&#039; \\int_{\\Gamma} |f|^2 dS.\nThus we have\n\\begin{aligned}\n\\mu_k &amp;\\ge \\min_{f\\in U^{\\perp} - \\{0\\}} \\frac{(1+O(r))\\int_{\\Gamma} |\\nabla_{\\Gamma} f(x)|^2 dS + O(r) \\int_{\\Gamma} |\\nabla_{\\Gamma} f|^2 dS }{\\int_{\\Gamma} |f(x)|^2 dS} \\\\&amp;= (1 + O(r)) \\lambda_k.\n\\end{aligned}\n\n\nNotes\n\n\n\nLinks\n\n\n"}}